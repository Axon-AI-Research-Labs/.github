ğŸŒŒ Welcome to Axon: AI Research Lab ğŸš€ğŸ¤–

This organization is a collaborative platform dedicated to:
- âœ¨ Implementing research papers in artificial intelligence.
- ğŸ§  Conducting our own original research projects.
- ğŸ“‘ Publishing research papers to contribute to the global AI community.

Our mission is to bridge the gap between theory and practice, by providing:

ğŸ”¬ High-quality, reproducible implementations of seminal and contemporary AI works (e.g., InstructGPT, LLaMA, Transformers, Diffusion Models, RLHF, etc.)

ğŸ“ Novel contributions through our own research and publications.

Together, we aim to push forward the boundaries of artificial intelligence while keeping our work open, collaborative, and impactful. ğŸŒğŸ’¡

### **Recent Publications:**
1. **[ExpertRAG: Efficient RAG with Mixture of Experts -- Optimizing Context Retrieval for Adaptive LLM Responses ](https://arxiv.org/abs/2504.08744)**
2. **[Galvatron: Automatic Distributed Training for Large Transformer Models](https://arxiv.org/abs/2504.03662)**
3. **[Theoretical Foundations and Mitigation of Hallucination in Large Language Models ](https://arxiv.org/abs/2507.22915)**
4. **[Mixture of Transformers: Macro-Level Gating for Sparse Activation in Large Language Model Ensembles](http://dx.doi.org/10.13140/RG.2.2.25049.02400)**
5. **[Universal Approximation Theorem for a Single-Layer Transformer](https://arxiv.org/abs/2507.10581)**

## ğŸ“š Featured Publications  

<table>
<tr>
  <td width="33%">
  <a href="https://arxiv.org/abs/2504.08744">
    <img src="https://github.com/Axon-AI-Research-Labs/assists-/blob/main/ExpertRAG.jpg" width="100%">
  </a>
  <p align="center"><b>ExpertRAG: Efficient RAG with Mixture of Experts -- Optimizing Context Retrieval for Adaptive LLM Responses</b></p>
</td>
<td width="33%">
  <a href="https://arxiv.org/abs/2507.22915">
    <img src="https://github.com/Axon-AI-Research-Labs/assists-/blob/main/Math.jpg" width="100%">
  </a>
  <p align="center"><b>Mitigation of Hallucination in LLM</b></p>
</td>
<td width="33%">
  <a href="https://arxiv.org/abs/2507.10581">
    <img src="https://github.com/Axon-AI-Research-Labs/assists-/blob/main/theory.jpg" width="100%">
  </a>
  <p align="center"><b>Universal Approximation in Transformers</b></p>
</td>
<td width="33%">
  <a href="https://arxiv.org/abs/2504.03662">
    <img src="https://github.com/Axon-AI-Research-Labs/assists-/blob/main/Galvatron.jpg" width="100%">
  </a>
  <p align="center"><b>Galvatron: Large Transformer Training</b></p>
</td>
</tr>
</table>
