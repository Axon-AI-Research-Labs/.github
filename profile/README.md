🌌 Welcome to Axon: AI Research Lab 🚀🤖

This organization is a collaborative platform dedicated to:
- ✨ Implementing research papers in artificial intelligence.
- 🧠 Conducting our own original research projects.
- 📑 Publishing research papers to contribute to the global AI community.

Our mission is to bridge the gap between theory and practice, by providing:

🔬 High-quality, reproducible implementations of seminal and contemporary AI works (e.g., InstructGPT, LLaMA, Transformers, Diffusion Models, RLHF, etc.)

📝 Novel contributions through our own research and publications.

Together, we aim to push forward the boundaries of artificial intelligence while keeping our work open, collaborative, and impactful. 🌍💡


## 📚 Featured Publications  

<table>
<tr>
  <td width="33%">
    <a href="https://arxiv.org/abs/2504.08744">
      <img src="https://github.com/Axon-AI-Research-Labs/assists-/blob/main/ExpertRAG.jpg" width="100%" height="400">
    </a>
    <p align="center"><b>ExpertRAG: Efficient RAG with Mixture of Experts -- Optimizing Context Retrieval for Adaptive LLM Responses</b></p>
  </td>
  <td width="33%">
    <a href="https://arxiv.org/abs/2507.22915">
      <img src="https://github.com/Axon-AI-Research-Labs/assists-/blob/main/Math.jpg" width="100%" height="400">
    </a>
    <p align="center"><b>Theoretical Foundations and Mitigation of Hallucination in Large Language Models</b></p>
  </td>
  <td width="33%">
    <a href="https://arxiv.org/abs/2507.10581">
      <img src="https://github.com/Axon-AI-Research-Labs/assists-/blob/main/theory.jpg" width="100%" height="400">
    </a>
    <p align="center"><b>Universal Approximation Theorem for a Single-Layer Transformer</b></p>
  </td>
</tr>
<tr>
  <td width="33%">
    <a href="https://arxiv.org/abs/2504.03662">
      <img src="https://github.com/Axon-AI-Research-Labs/assists-/blob/main/Galvatron.jpg" width="100%" height="400">
    </a>
    <p align="center"><b>Galvatron: Automatic Distributed Training for Large Transformer Models</b></p>
  </td>
  <td width="33%">
    <a href="http://dx.doi.org/10.13140/RG.2.2.25049.02400">
      <img src="https://github.com/Axon-AI-Research-Labs/assists-/blob/main/MOT.jpg" width="100%" height="400">
    </a>
    <p align="center"><b>Mixture of Transformers: Macro-Level Gating for Sparse Activation in LLM Ensembles</b></p>
  </td>
  <td width="33%">
    <a href="http://dx.doi.org/10.13140/RG.2.2.22814.24643">
      <img src="https://github.com/Axon-AI-Research-Labs/assists-/blob/main/thesis.jpg" width="100%" height="400">
    </a>
    <p align="center"><b>Bachelor Thesis: AI Engine – Deep Learning and Neural Network Engine</b></p>
  </td>
</tr>
</table>
