🌌 Welcome to Axon: AI Research Lab 🚀🤖

This organization is a collaborative platform dedicated to:
- ✨ Implementing research papers in artificial intelligence.
- 🧠 Conducting our own original research projects.
- 📑 Publishing research papers to contribute to the global AI community.

Our mission is to bridge the gap between theory and practice, by providing:

🔬 High-quality, reproducible implementations of seminal and contemporary AI works (e.g., InstructGPT, LLaMA, Transformers, Diffusion Models, RLHF, etc.)

📝 Novel contributions through our own research and publications.

Together, we aim to push forward the boundaries of artificial intelligence while keeping our work open, collaborative, and impactful. 🌍💡

### **Recent Publications:**
1. **[ExpertRAG: Efficient RAG with Mixture of Experts -- Optimizing Context Retrieval for Adaptive LLM Responses ](https://arxiv.org/abs/2504.08744)**
2. **[Galvatron: Automatic Distributed Training for Large Transformer Models](https://arxiv.org/abs/2504.03662)**
3. **[Theoretical Foundations and Mitigation of Hallucination in Large Language Models ](https://arxiv.org/abs/2507.22915)**
4. **[Mixture of Transformers: Macro-Level Gating for Sparse Activation in Large Language Model Ensembles](http://dx.doi.org/10.13140/RG.2.2.25049.02400)**
5. **[Universal Approximation Theorem for a Single-Layer Transformer](https://arxiv.org/abs/2507.10581)**

## 📚 Featured Publications  

<table>
<tr>
  <td width="33%">
  <a href="https://arxiv.org/abs/2504.08744">
    <img src="https://github.com/Axon-AI-Research-Labs/assists-/blob/main/ExpertRAG.jpg" width="100%">
  </a>
  <p align="center"><b>ExpertRAG: Efficient RAG with Mixture of Experts -- Optimizing Context Retrieval for Adaptive LLM Responses</b></p>
</td>
<td width="33%">
  <a href="https://arxiv.org/abs/2507.22915">
    <img src="https://github.com/Axon-AI-Research-Labs/assists-/blob/main/Math.jpg" width="100%">
  </a>
  <p align="center"><b>Mitigation of Hallucination in LLM</b></p>
</td>
<td width="33%">
  <a href="https://arxiv.org/abs/2507.10581">
    <img src="https://github.com/Axon-AI-Research-Labs/assists-/blob/main/theory.jpg" width="100%">
  </a>
  <p align="center"><b>Universal Approximation in Transformers</b></p>
</td>
<td width="33%">
  <a href="https://arxiv.org/abs/2504.03662">
    <img src="https://github.com/Axon-AI-Research-Labs/assists-/blob/main/Galvatron.jpg" width="100%">
  </a>
  <p align="center"><b>Galvatron: Large Transformer Training</b></p>
</td>
</tr>
</table>
